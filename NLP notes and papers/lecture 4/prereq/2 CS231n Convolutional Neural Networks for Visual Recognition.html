<!DOCTYPE html>
<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>CS231n Convolutional Neural Networks for Visual Recognition</title>
    <meta name="viewport" content="width=device-width">
    <meta name="description" content="Course materials and notes for Stanford class CS231n: Convolutional Neural Networks for Visual Recognition.">
    <link rel="canonical" href="http://cs231n.github.io/neural-networks-1/">

    <!-- Custom CSS -->
    <link rel="stylesheet" href="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/main.css">

    <!-- Google fonts -->
    <link href="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/css.css" rel="stylesheet" type="text/css">

    <!-- Google tracking -->
    <script async="" src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/analytics.js"></script><script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-46895817-2', 'auto');
      ga('send', 'pageview');

    </script>
    
<style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1px; bottom: 2px; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">.MathJax_Display {text-align: center; margin: 1em 0em; position: relative; display: block!important; text-indent: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; width: 100%}
.MathJax .merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MathJax .MJX-monospace {font-family: monospace}
.MathJax .MJX-sans-serif {font-family: sans-serif}
#MathJax_Tooltip {background-color: InfoBackground; color: InfoText; border: 1px solid black; box-shadow: 2px 2px 5px #AAAAAA; -webkit-box-shadow: 2px 2px 5px #AAAAAA; -moz-box-shadow: 2px 2px 5px #AAAAAA; -khtml-box-shadow: 2px 2px 5px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true'); padding: 3px 4px; z-index: 401; position: absolute; left: 0; top: 0; width: auto; height: auto; display: none}
.MathJax {display: inline; font-style: normal; font-weight: normal; line-height: normal; font-size: 100%; font-size-adjust: none; text-indent: 0; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0; min-height: 0; border: 0; padding: 0; margin: 0}
.MathJax:focus, body :focus .MathJax {display: inline-table}
.MathJax.MathJax_FullWidth {text-align: center; display: table-cell!important; width: 10000em!important}
.MathJax img, .MathJax nobr, .MathJax a {border: 0; padding: 0; margin: 0; max-width: none; max-height: none; min-width: 0; min-height: 0; vertical-align: 0; line-height: normal; text-decoration: none}
img.MathJax_strut {border: 0!important; padding: 0!important; margin: 0!important; vertical-align: 0!important}
.MathJax span {display: inline; position: static; border: 0; padding: 0; margin: 0; vertical-align: 0; line-height: normal; text-decoration: none}
.MathJax nobr {white-space: nowrap!important}
.MathJax img {display: inline!important; float: none!important}
.MathJax * {transition: none; -webkit-transition: none; -moz-transition: none; -ms-transition: none; -o-transition: none}
.MathJax_Processing {visibility: hidden; position: fixed; width: 0; height: 0; overflow: hidden}
.MathJax_Processed {display: none!important}
.MathJax_ExBox {display: block!important; overflow: hidden; width: 1px; height: 60ex; min-height: 0; max-height: none}
.MathJax .MathJax_EmBox {display: block!important; overflow: hidden; width: 1px; height: 60em; min-height: 0; max-height: none}
.MathJax_LineBox {display: table!important}
.MathJax_LineBox span {display: table-cell!important; width: 10000em!important; min-width: 0; max-width: none; padding: 0; border: 0; margin: 0}
.MathJax .MathJax_HitBox {cursor: text; background: white; opacity: 0; filter: alpha(opacity=0)}
.MathJax .MathJax_HitBox * {filter: none; opacity: 1; background: transparent}
#MathJax_Tooltip * {filter: none; opacity: 1; background: transparent}
@font-face {font-family: MathJax_Blank; src: url('about:blank')}
.MathJax .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style></head>


    <body><div style="visibility: hidden; overflow: hidden; position: absolute; top: 0px; height: 1px; width: auto; padding: 0px; border: 0px none; margin: 0px; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal;"><div id="MathJax_Hidden"></div></div><div id="MathJax_Message" style="display: none;"></div>

    <header class="site-header">

  <div class="wrap title-wrap">
    <a class="site-title" href="http://cs231n.github.io/">CS231n Convolutional Neural Networks for Visual Recognition</a>
  </div>

</header>


    <div class="page-content">
      <div class="wrap">
      <div class="post">

  <header class="post-header">
    <h1></h1>
  </header>

  <article class="post-content">
  <p>Table of Contents:</p>

<ul>
  <li><a href="#quick">Quick intro without brain analogies</a></li>
  <li><a href="#intro">Modeling one neuron</a>
    <ul>
      <li><a href="#bio">Biological motivation and connections</a></li>
      <li><a href="#classifier">Single neuron as a linear classifier</a></li>
      <li><a href="#actfun">Commonly used activation functions</a></li>
    </ul>
  </li>
  <li><a href="#nn">Neural Network architectures</a>
    <ul>
      <li><a href="#layers">Layer-wise organization</a></li>
      <li><a href="#feedforward">Example feed-forward computation</a></li>
      <li><a href="#power">Representational power</a></li>
      <li><a href="#arch">Setting number of layers and their sizes</a></li>
    </ul>
  </li>
  <li><a href="#summary">Summary</a></li>
  <li><a href="#add">Additional references</a></li>
</ul>

<p><a name="quick"></a></p>

<h2 id="quick-intro">Quick intro</h2>

<p>It is possible to introduce neural networks without appealing to 
brain analogies. In the section on linear classification we computed 
scores for different visual categories given the image using the formula
 <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-1-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-1" style="width: 3.945em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.029em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.751em, 1003.03em, 2.71em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-2"><span class="mi" id="MathJax-Span-3" style="font-family: STIXGeneral; font-style: italic;">s</span><span class="mo" id="MathJax-Span-4" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mi" id="MathJax-Span-5" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.313em;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span class="mi" id="MathJax-Span-6" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.086em; border-left: 0px solid; width: 0px; height: 0.997em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>=</mo><mi>W</mi><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-1"> s = W x </script>, where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-2-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-7" style="width: 1.204em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.913em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.751em, 1000.91em, 2.71em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-8"><span class="mi" id="MathJax-Span-9" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.086em; border-left: 0px solid; width: 0px; height: 0.997em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-2">W</script> was a matrix and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-3-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-10" style="width: 0.579em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.433em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.963em, 1000.43em, 2.703em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-11"><span class="mi" id="MathJax-Span-12" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.077em; border-left: 0px solid; width: 0px; height: 0.713em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-3">x</script> was an input column vector containing all pixel data of the image. In the case of CIFAR-10, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-4-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-13" style="width: 0.579em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.433em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.963em, 1000.43em, 2.703em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-14"><span class="mi" id="MathJax-Span-15" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.077em; border-left: 0px solid; width: 0px; height: 0.713em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi></math></span></span><script type="math/tex" id="MathJax-Element-4">x</script> is a [3072x1] column vector, and <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-5-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-16" style="width: 1.204em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.913em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.751em, 1000.91em, 2.71em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-17"><span class="mi" id="MathJax-Span-18" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.086em; border-left: 0px solid; width: 0px; height: 0.997em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>W</mi></math></span></span><script type="math/tex" id="MathJax-Element-5">W</script> is a [10x3072] matrix, so that the output scores is a vector of 10 class scores.</p>

<p>An example neural network would instead compute <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-6-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-19" style="width: 10.531em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.077em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.728em, 1008.03em, 2.869em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-20"><span class="mi" id="MathJax-Span-21" style="font-family: STIXGeneral; font-style: italic;">s</span><span class="mo" id="MathJax-Span-22" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="msubsup" id="MathJax-Span-23" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-24" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-25" style="font-size: 70.7%; font-family: STIXGeneral;">2</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-26" style="font-family: STIXGeneral; padding-left: 0.188em;">max</span><span class="mo" id="MathJax-Span-27" style="font-family: STIXGeneral;">(</span><span class="mn" id="MathJax-Span-28" style="font-family: STIXGeneral;">0</span><span class="mo" id="MathJax-Span-29" style="font-family: STIXGeneral;">,</span><span class="msubsup" id="MathJax-Span-30" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-31" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-32" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mi" id="MathJax-Span-33" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-34" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.293em; border-left: 0px solid; width: 0px; height: 1.234em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>=</mo><msub><mi>W</mi><mn>2</mn></msub><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msub><mi>W</mi><mn>1</mn></msub><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-6"> s = W_2 \max(0, W_1 x) </script>. Here, <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-7-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-35" style="width: 1.637em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.25em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.366em, 1001.25em, 2.458em, -1000em); top: -2.163em; left: 0em;"><span class="mrow" id="MathJax-Span-36"><span class="msubsup" id="MathJax-Span-37"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-38" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-39" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.163em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.258em; border-left: 0px solid; width: 0px; height: 1.169em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>W</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-7">W_1</script> could be, for example, a [100x3072] matrix transforming the image into a 100-dimensional intermediate vector. The function <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-8-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-40" style="width: 5.099em; display: inline-block;"><span style="display: inline-block; position: relative; width: 3.894em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.728em, 1003.85em, 2.869em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-41"><span class="mi" id="MathJax-Span-42" style="font-family: STIXGeneral; font-style: italic;">m</span><span class="mi" id="MathJax-Span-43" style="font-family: STIXGeneral; font-style: italic;">a</span><span class="mi" id="MathJax-Span-44" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-45" style="font-family: STIXGeneral;">(</span><span class="mn" id="MathJax-Span-46" style="font-family: STIXGeneral;">0</span><span class="mo" id="MathJax-Span-47" style="font-family: STIXGeneral;">,</span><span class="mo" id="MathJax-Span-48" style="font-family: STIXGeneral; padding-left: 0.188em;">−</span><span class="mo" id="MathJax-Span-49" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.293em; border-left: 0px solid; width: 0px; height: 1.234em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>m</mi><mi>a</mi><mi>x</mi><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mo>−</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-8">max(0,-) </script>
 is a non-linearity that is applied elementwise. There are several 
choices we could make for the non-linearity (which we’ll study below), 
but this one is a common choice and simply thresholds all activations 
that are below zero to zero. Finally, the matrix <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-9-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-50" style="width: 1.637em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.25em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.366em, 1001.25em, 2.458em, -1000em); top: -2.163em; left: 0em;"><span class="mrow" id="MathJax-Span-51"><span class="msubsup" id="MathJax-Span-52"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-53" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-54" style="font-size: 70.7%; font-family: STIXGeneral;">2</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.163em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.258em; border-left: 0px solid; width: 0px; height: 1.169em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>W</mi><mn>2</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-9">W_2</script>
 would then be of size [10x100], so that we again get 10 numbers out 
that we interpret as the class scores. Notice that the non-linearity is 
critical computationally - if we left it out, the two matrices could be 
collapsed to a single matrix, and therefore the predicted class scores 
would again be a linear function of the input. The non-linearity is 
where we get the <em>wiggle</em>. The parameters <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-10-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-55" style="width: 3.849em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.933em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.751em, 1002.93em, 2.842em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-56"><span class="msubsup" id="MathJax-Span-57"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-58" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-59" style="font-size: 70.7%; font-family: STIXGeneral;">2</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-60" style="font-family: STIXGeneral;">,</span><span class="msubsup" id="MathJax-Span-61" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-62" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-63" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.258em; border-left: 0px solid; width: 0px; height: 1.169em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>W</mi><mn>2</mn></msub><mo>,</mo><msub><mi>W</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-10">W_2, W_1</script>
 are learned with stochastic gradient descent, and their gradients are 
derived with chain rule (and computed with backpropagation).</p>

<p>A three-layer neural network could analogously look like <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-11-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-64" style="width: 16.685em; display: inline-block;"><span style="display: inline-block; position: relative; width: 12.837em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.728em, 1012.79em, 2.869em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-65"><span class="mi" id="MathJax-Span-66" style="font-family: STIXGeneral; font-style: italic;">s</span><span class="mo" id="MathJax-Span-67" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="msubsup" id="MathJax-Span-68" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-69" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-70" style="font-size: 70.7%; font-family: STIXGeneral;">3</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-71" style="font-family: STIXGeneral; padding-left: 0.188em;">max</span><span class="mo" id="MathJax-Span-72" style="font-family: STIXGeneral;">(</span><span class="mn" id="MathJax-Span-73" style="font-family: STIXGeneral;">0</span><span class="mo" id="MathJax-Span-74" style="font-family: STIXGeneral;">,</span><span class="msubsup" id="MathJax-Span-75" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-76" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-77" style="font-size: 70.7%; font-family: STIXGeneral;">2</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-78" style="font-family: STIXGeneral; padding-left: 0.188em;">max</span><span class="mo" id="MathJax-Span-79" style="font-family: STIXGeneral;">(</span><span class="mn" id="MathJax-Span-80" style="font-family: STIXGeneral;">0</span><span class="mo" id="MathJax-Span-81" style="font-family: STIXGeneral;">,</span><span class="msubsup" id="MathJax-Span-82" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-83" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-84" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mi" id="MathJax-Span-85" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-86" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-87" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.293em; border-left: 0px solid; width: 0px; height: 1.234em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>s</mi><mo>=</mo><msub><mi>W</mi><mn>3</mn></msub><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msub><mi>W</mi><mn>2</mn></msub><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><msub><mi>W</mi><mn>1</mn></msub><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-11"> s = W_3 \max(0, W_2 \max(0, W_1 x)) </script>, where all of <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-12-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;3&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-88" style="width: 6.06em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.663em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.751em, 1004.66em, 2.852em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-89"><span class="msubsup" id="MathJax-Span-90"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-91" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-92" style="font-size: 70.7%; font-family: STIXGeneral;">3</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-93" style="font-family: STIXGeneral;">,</span><span class="msubsup" id="MathJax-Span-94" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-95" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-96" style="font-size: 70.7%; font-family: STIXGeneral;">2</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-97" style="font-family: STIXGeneral;">,</span><span class="msubsup" id="MathJax-Span-98" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.262em; height: 0px;"><span style="position: absolute; clip: rect(3.193em, 1000.91em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-99" style="font-family: STIXGeneral; font-style: italic;">W<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.073em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.833em;"><span class="mn" id="MathJax-Span-100" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.27em; border-left: 0px solid; width: 0px; height: 1.182em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>W</mi><mn>3</mn></msub><mo>,</mo><msub><mi>W</mi><mn>2</mn></msub><mo>,</mo><msub><mi>W</mi><mn>1</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-12">W_3, W_2, W_1</script>
 are parameters to be learned. The sizes of the intermediate hidden 
vectors are hyperparameters of the network and we’ll see how we can set 
them later. Lets now look into how we can interpret these computations 
from the neuron/network perspective.</p>

<p><a name="intro"></a></p>

<h2 id="modeling-one-neuron">Modeling one neuron</h2>

<p>The area of Neural Networks has originally been primarily inspired by
 the goal of modeling biological neural systems, but has since diverged 
and become a matter of engineering and achieving good results in Machine
 Learning tasks. Nonetheless, we begin our discussion with a very brief 
and high-level description of the biological system that a large portion
 of this area has been inspired by.</p>

<p><a name="bio"></a></p>

<h3 id="biological-motivation-and-connections">Biological motivation and connections</h3>

<p>The basic computational unit of the brain is a <strong>neuron</strong>.
 Approximately 86 billion neurons can be found in the human nervous 
system and they are connected with approximately 10^14 - 10^15 <strong>synapses</strong>.
 The diagram below shows a cartoon drawing of a biological neuron (left)
 and a common mathematical model (right). Each neuron receives input 
signals from its <strong>dendrites</strong> and produces output signals along its (single) <strong>axon</strong>.
 The axon eventually branches out and connects via synapses to dendrites
 of other neurons. In the computational model of a neuron, the signals 
that travel along the axons (e.g. <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-13-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-101" style="width: 1.156em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.865em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.578em, 1000.86em, 2.468em, -1000em); top: -2.163em; left: 0em;"><span class="mrow" id="MathJax-Span-102"><span class="msubsup" id="MathJax-Span-103"><span style="display: inline-block; position: relative; width: 0.873em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.45em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-104" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mn" id="MathJax-Span-105" style="font-size: 70.7%; font-family: STIXGeneral;">0</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.163em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.27em; border-left: 0px solid; width: 0px; height: 0.906em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>x</mi><mn>0</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-13">x_0</script>) interact multiplicatively (e.g. <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-14-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-106" style="width: 2.599em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.971em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.578em, 1001.97em, 2.468em, -1000em); top: -2.163em; left: 0em;"><span class="mrow" id="MathJax-Span-107"><span class="msubsup" id="MathJax-Span-108"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.65em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-109" style="font-family: STIXGeneral; font-style: italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.667em;"><span class="mn" id="MathJax-Span-110" style="font-size: 70.7%; font-family: STIXGeneral;">0</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-111"><span style="display: inline-block; position: relative; width: 0.873em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.45em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-112" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mn" id="MathJax-Span-113" style="font-size: 70.7%; font-family: STIXGeneral;">0</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.163em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.27em; border-left: 0px solid; width: 0px; height: 0.906em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>w</mi><mn>0</mn></msub><msub><mi>x</mi><mn>0</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-14">w_0 x_0</script>) with the dendrites of the other neuron based on the synaptic strength at that synapse (e.g. <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-15-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/msub&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-114" style="width: 1.445em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.106em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.578em, 1001.11em, 2.468em, -1000em); top: -2.163em; left: 0em;"><span class="mrow" id="MathJax-Span-115"><span class="msubsup" id="MathJax-Span-116"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.65em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-117" style="font-family: STIXGeneral; font-style: italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.667em;"><span class="mn" id="MathJax-Span-118" style="font-size: 70.7%; font-family: STIXGeneral;">0</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span></span><span style="display: inline-block; width: 0px; height: 2.163em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.27em; border-left: 0px solid; width: 0px; height: 0.906em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>w</mi><mn>0</mn></msub></math></span></span><script type="math/tex" id="MathJax-Element-15">w_0</script>). The idea is that the synaptic strengths (the weights <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-16-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-119" style="width: 0.916em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.673em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.963em, 1000.65em, 2.71em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-120"><span class="mi" id="MathJax-Span-121" style="font-family: STIXGeneral; font-style: italic;">w</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.086em; border-left: 0px solid; width: 0px; height: 0.722em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi></math></span></span><script type="math/tex" id="MathJax-Element-16">w</script>)
 are learnable and control the strength of influence (and its direction:
 excitory (positive weight) or inhibitory (negative weight)) of one 
neuron on another. In the basic model, the dendrites carry the signal to
 the cell body where they all get summed. If the final sum is above a 
certain threshold, the neuron can <em>fire</em>, sending a spike along 
its axon. In the computational model, we assume that the precise timings
 of the spikes do not matter, and that only the frequency of the firing 
communicates information. Based on this <em>rate code</em> interpretation, we model the <em>firing rate</em> of the neuron with an <strong>activation function</strong> <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-17-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-122" style="width: 0.579em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.433em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.726em, 1000.43em, 2.899em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-123"><span class="mi" id="MathJax-Span-124" style="font-family: STIXGeneral; font-style: italic; text-rendering: optimizelegibility;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.332em; border-left: 0px solid; width: 0px; height: 1.276em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></span></span><script type="math/tex" id="MathJax-Element-17">f</script>, which represents the frequency of the spikes along the axon. Historically, a common choice of activation function is the <strong>sigmoid function</strong> <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-18-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-125" style="width: 0.724em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.529em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.976em, 1000.53em, 2.703em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-126"><span class="mi" id="MathJax-Span-127" style="font-family: STIXGeneral; font-style: italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.033em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.077em; border-left: 0px solid; width: 0px; height: 0.696em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>σ</mi></math></span></span><script type="math/tex" id="MathJax-Element-18">\sigma</script>,
 since it takes a real-valued input (the signal strength after the sum) 
and squashes it to range between 0 and 1. We will see details of these 
activation functions later in this section.</p>

<div class="fig figcenter fighighlight">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/neuron.png" width="49%">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/neuron_model.jpeg" style="border-left: 1px solid black;" width="49%">
  <div class="figcaption">A cartoon drawing of a biological neuron (left) and its mathematical model (right).</div>
</div>

<p>An example code for forward-propagating a single neuron might look as follows:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">class</span> <span class="nc">Neuron</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
  <span class="c"># ... </span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">inputs</span><span class="p">):</span>
    <span class="s">""" assume inputs and weights are 1-D numpy arrays and bias is a number """</span>
    <span class="n">cell_body_sum</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">weights</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
    <span class="n">firing_rate</span> <span class="o">=</span> <span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">math</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">cell_body_sum</span><span class="p">))</span> <span class="c"># sigmoid activation function</span>
    <span class="k">return</span> <span class="n">firing_rate</span>
</code></pre>
</div>

<p>In other words, each neuron performs a dot product with the input and
 its weights, adds the bias and applies the non-linearity (or activation
 function), in this case the sigmoid <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-19-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-128" style="width: 9.57em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.356em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.654em, 1007.31em, 2.869em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-129"><span class="mi" id="MathJax-Span-130" style="font-family: STIXGeneral; font-style: italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.033em;"></span></span><span class="mo" id="MathJax-Span-131" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-132" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-133" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-134" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-135" style="font-family: STIXGeneral; padding-left: 0.313em;">1</span><span class="texatom" id="MathJax-Span-136"><span class="mrow" id="MathJax-Span-137"><span class="mo" id="MathJax-Span-138" style="font-family: STIXGeneral;">/</span></span></span><span class="mo" id="MathJax-Span-139" style="font-family: STIXGeneral;">(</span><span class="mn" id="MathJax-Span-140" style="font-family: STIXGeneral;">1</span><span class="mo" id="MathJax-Span-141" style="font-family: STIXGeneral; padding-left: 0.25em;">+</span><span class="msubsup" id="MathJax-Span-142" style="padding-left: 0.25em;"><span style="display: inline-block; position: relative; width: 1.319em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.41em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-143" style="font-family: STIXGeneral; font-style: italic;">e</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -4.353em; left: 0.444em;"><span class="texatom" id="MathJax-Span-144"><span class="mrow" id="MathJax-Span-145"><span class="mo" id="MathJax-Span-146" style="font-size: 70.7%; font-family: STIXGeneral;">−</span><span class="mi" id="MathJax-Span-147" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-148" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.293em; border-left: 0px solid; width: 0px; height: 1.33em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mi>x</mi></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-19">\sigma(x) = 1/(1+e^{-x})</script>. We will go into more details about different activation functions at the end of this section.</p>

<p><strong>Coarse model.</strong> It’s important to stress that this 
model of a biological neuron is very coarse: For example, there are many
 different types of neurons, each with different properties. The 
dendrites in biological neurons perform complex nonlinear computations. 
The synapses are not just a single weight, they’re a complex non-linear 
dynamical system. The exact timing of the output spikes in many systems 
in known to be important, suggesting that the rate code approximation 
may not hold. Due to all these and many other simplifications, be 
prepared to hear groaning sounds from anyone with some neuroscience 
background if you draw analogies between Neural Networks and real 
brains. See this <a href="https://physics.ucsd.edu/neurophysics/courses/physics_171/annurev.neuro.28.061604.135703.pdf">review</a> (pdf), or more recently this <a href="http://www.sciencedirect.com/science/article/pii/S0959438814000130">review</a> if you are interested.</p>

<p><a name="classifier"></a></p>

<h3 id="single-neuron-as-a-linear-classifier">Single neuron as a linear classifier</h3>

<p>The mathematical form of the model Neuron’s forward computation might
 look familiar to you. As we saw with linear classifiers, a neuron has 
the capacity to “like” (activation near one) or “dislike” (activation 
near zero) certain linear regions of its input space. Hence, with an 
appropriate loss function on the neuron’s output, we can turn a single 
neuron into a linear classifier:</p>

<p><strong>Binary Softmax classifier</strong>. For example, we can interpret <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-20-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-149" style="width: 7.647em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5.865em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.643em, 1005.82em, 2.996em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-150"><span class="mi" id="MathJax-Span-151" style="font-family: STIXGeneral; font-style: italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.033em;"></span></span><span class="mo" id="MathJax-Span-152" style="font-family: STIXGeneral;">(</span><span class="munderover" id="MathJax-Span-153"><span style="display: inline-block; position: relative; width: 1.186em; height: 0px;"><span style="position: absolute; clip: rect(3.085em, 1000.86em, 4.396em, -1000em); top: -3.99em; left: 0em;"><span class="mo" id="MathJax-Span-154" style="font-family: STIXGeneral; vertical-align: -0.002em;">∑</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.694em; left: 0.914em;"><span class="mi" id="MathJax-Span-155" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-156" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 0.939em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.65em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-157" style="font-family: STIXGeneral; font-style: italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.667em;"><span class="mi" id="MathJax-Span-158" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-159"><span style="display: inline-block; position: relative; width: 0.716em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.45em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-160" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mi" id="MathJax-Span-161" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-162" style="font-family: STIXGeneral; padding-left: 0.25em;">+</span><span class="mi" id="MathJax-Span-163" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.25em;">b</span><span class="mo" id="MathJax-Span-164" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.458em; border-left: 0px solid; width: 0px; height: 1.51em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>σ</mi><mo stretchy="false">(</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>w</mi><mi>i</mi></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-20">\sigma(\sum_iw_ix_i + b)</script> to be the probability of one of the classes <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-21-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x2223;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-165" style="width: 8.56em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.587em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.714em, 1006.54em, 2.898em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-166"><span class="mi" id="MathJax-Span-167" style="font-family: STIXGeneral; font-style: italic;">P</span><span class="mo" id="MathJax-Span-168" style="font-family: STIXGeneral;">(</span><span class="msubsup" id="MathJax-Span-169"><span style="display: inline-block; position: relative; width: 0.716em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.43em, 4.341em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-170" style="font-family: STIXGeneral; font-style: italic;">y</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mi" id="MathJax-Span-171" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-172" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-173" style="font-family: STIXGeneral; padding-left: 0.313em;">1</span><span class="mo" id="MathJax-Span-174" style="font-family: STIXGeneral; padding-left: 0.313em;">∣</span><span class="msubsup" id="MathJax-Span-175" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 0.716em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.45em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-176" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mi" id="MathJax-Span-177" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-178" style="font-family: STIXGeneral;">;</span><span class="mi" id="MathJax-Span-179" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.188em;">w</span><span class="mo" id="MathJax-Span-180" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.33em; border-left: 0px solid; width: 0px; height: 1.29em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo>;</mo><mi>w</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-21">P(y_i = 1 \mid x_i; w) </script>. The probability of the other class would be <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-22-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;&amp;#x2223;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;y&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;&amp;#x2223;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;;&lt;/mo&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-181" style="width: 20.964em; display: inline-block;"><span style="display: inline-block; position: relative; width: 16.106em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.714em, 1016.06em, 2.898em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-182"><span class="mi" id="MathJax-Span-183" style="font-family: STIXGeneral; font-style: italic;">P</span><span class="mo" id="MathJax-Span-184" style="font-family: STIXGeneral;">(</span><span class="msubsup" id="MathJax-Span-185"><span style="display: inline-block; position: relative; width: 0.716em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.43em, 4.341em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-186" style="font-family: STIXGeneral; font-style: italic;">y</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mi" id="MathJax-Span-187" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-188" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-189" style="font-family: STIXGeneral; padding-left: 0.313em;">0</span><span class="mo" id="MathJax-Span-190" style="font-family: STIXGeneral; padding-left: 0.313em;">∣</span><span class="msubsup" id="MathJax-Span-191" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 0.716em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.45em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-192" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mi" id="MathJax-Span-193" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-194" style="font-family: STIXGeneral;">;</span><span class="mi" id="MathJax-Span-195" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.188em;">w</span><span class="mo" id="MathJax-Span-196" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-197" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-198" style="font-family: STIXGeneral; padding-left: 0.313em;">1</span><span class="mo" id="MathJax-Span-199" style="font-family: STIXGeneral; padding-left: 0.25em;">−</span><span class="mi" id="MathJax-Span-200" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.25em;">P</span><span class="mo" id="MathJax-Span-201" style="font-family: STIXGeneral;">(</span><span class="msubsup" id="MathJax-Span-202"><span style="display: inline-block; position: relative; width: 0.716em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.43em, 4.341em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-203" style="font-family: STIXGeneral; font-style: italic;">y</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mi" id="MathJax-Span-204" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-205" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-206" style="font-family: STIXGeneral; padding-left: 0.313em;">1</span><span class="mo" id="MathJax-Span-207" style="font-family: STIXGeneral; padding-left: 0.313em;">∣</span><span class="msubsup" id="MathJax-Span-208" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 0.716em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.45em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-209" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mi" id="MathJax-Span-210" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-211" style="font-family: STIXGeneral;">;</span><span class="mi" id="MathJax-Span-212" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.188em;">w</span><span class="mo" id="MathJax-Span-213" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.33em; border-left: 0px solid; width: 0px; height: 1.29em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mn>0</mn><mo>∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo>;</mo><mi>w</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mo>−</mo><mi>P</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>=</mo><mn>1</mn><mo>∣</mo><msub><mi>x</mi><mi>i</mi></msub><mo>;</mo><mi>w</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-22">P(y_i = 0 \mid x_i; w) = 1 - P(y_i = 1 \mid x_i; w) </script>,
 since they must sum to one. With this interpretation, we can formulate 
the cross-entropy loss as we have seen in the Linear Classification 
section, and optimizing it would lead to a binary Softmax classifier 
(also known as <em>logistic regression</em>). Since the sigmoid function
 is restricted to be between 0-1, the predictions of this classifier are
 based on whether the output of the neuron is greater than 0.5.</p>

<p><strong>Binary SVM classifier</strong>. Alternatively, we could 
attach a max-margin hinge loss to the output of the neuron and train it 
to become a binary Support Vector Machine.</p>

<p><strong>Regularization interpretation</strong>. The regularization loss in both SVM/Softmax cases could in this biological view be interpreted as <em>gradual forgetting</em>, since it would have the effect of driving all synaptic weights <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-23-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-214" style="width: 0.916em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.673em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.963em, 1000.65em, 2.71em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-215"><span class="mi" id="MathJax-Span-216" style="font-family: STIXGeneral; font-style: italic;">w</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.086em; border-left: 0px solid; width: 0px; height: 0.722em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi></math></span></span><script type="math/tex" id="MathJax-Element-23">w</script> towards zero after every parameter update.</p>

<blockquote>
  <p>A single neuron can be used to implement a binary classifier (e.g. binary Softmax or binary SVM classifiers)</p>
</blockquote>

<p><a name="actfun"></a></p>

<h3 id="commonly-used-activation-functions">Commonly used activation functions</h3>

<p>Every activation function (or <em>non-linearity</em>) takes a single 
number and performs a certain fixed mathematical operation on it. There 
are several activation functions you may encounter in practice:</p>

<div class="fig figcenter fighighlight">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/sigmoid.jpeg" width="40%">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/tanh.jpeg" style="border-left: 1px solid black;" width="40%">
  <div class="figcaption"><b>Left:</b> Sigmoid non-linearity squashes real numbers to range between [0,1] <b>Right:</b> The tanh non-linearity squashes real numbers to range between [-1,1].</div>
</div>

<p><strong>Sigmoid.</strong> The sigmoid non-linearity has the mathematical form <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-24-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-217" style="width: 9.57em; display: inline-block;"><span style="display: inline-block; position: relative; width: 7.356em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.654em, 1007.31em, 2.869em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-218"><span class="mi" id="MathJax-Span-219" style="font-family: STIXGeneral; font-style: italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.033em;"></span></span><span class="mo" id="MathJax-Span-220" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-221" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-222" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-223" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-224" style="font-family: STIXGeneral; padding-left: 0.313em;">1</span><span class="texatom" id="MathJax-Span-225"><span class="mrow" id="MathJax-Span-226"><span class="mo" id="MathJax-Span-227" style="font-family: STIXGeneral;">/</span></span></span><span class="mo" id="MathJax-Span-228" style="font-family: STIXGeneral;">(</span><span class="mn" id="MathJax-Span-229" style="font-family: STIXGeneral;">1</span><span class="mo" id="MathJax-Span-230" style="font-family: STIXGeneral; padding-left: 0.25em;">+</span><span class="msubsup" id="MathJax-Span-231" style="padding-left: 0.25em;"><span style="display: inline-block; position: relative; width: 1.319em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.41em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-232" style="font-family: STIXGeneral; font-style: italic;">e</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -4.353em; left: 0.444em;"><span class="texatom" id="MathJax-Span-233"><span class="mrow" id="MathJax-Span-234"><span class="mo" id="MathJax-Span-235" style="font-size: 70.7%; font-family: STIXGeneral;">−</span><span class="mi" id="MathJax-Span-236" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.002em;"></span></span></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-237" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.293em; border-left: 0px solid; width: 0px; height: 1.33em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>σ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>1</mn><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mo stretchy="false">(</mo><mn>1</mn><mo>+</mo><msup><mi>e</mi><mrow class="MJX-TeXAtom-ORD"><mo>−</mo><mi>x</mi></mrow></msup><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-24">\sigma(x) = 1 / (1 + e^{-x})</script>
 and is shown in the image above on the left. As alluded to in the 
previous section, it takes a real-valued number and “squashes” it into 
range between 0 and 1. In particular, large negative numbers become 0 
and large positive numbers become 1. The sigmoid function has seen 
frequent use historically since it has a nice interpretation as the 
firing rate of a neuron: from not firing at all (0) to fully-saturated 
firing at an assumed maximum frequency (1). In practice, the sigmoid 
non-linearity has recently fallen out of favor and it is rarely ever 
used. It has two major drawbacks:</p>

<ul>
  <li><em>Sigmoids saturate and kill gradients</em>. A very undesirable 
property of the sigmoid neuron is that when the neuron’s activation 
saturates at either tail of 0 or 1, the gradient at these regions is 
almost zero. Recall that during backpropagation, this (local) gradient 
will be multiplied to the gradient of this gate’s output for the whole 
objective. Therefore, if the local gradient is very small, it will 
effectively “kill” the gradient and almost no signal will flow through 
the neuron to its weights and recursively to its data. Additionally, one
 must pay extra caution when initializing the weights of sigmoid neurons
 to prevent saturation. For example, if the initial weights are too 
large then most neurons would become saturated and the network will 
barely learn.</li>
  <li><em>Sigmoid outputs are not zero-centered</em>. This is 
undesirable since neurons in later layers of processing in a Neural 
Network (more on this soon) would be receiving data that is not 
zero-centered. This has implications on the dynamics during gradient 
descent, because if the data coming into a neuron is always positive 
(e.g. <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-25-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-238" style="width: 2.887em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.212em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.728em, 1002.19em, 2.716em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-239"><span class="mi" id="MathJax-Span-240" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-241" style="font-family: STIXGeneral; padding-left: 0.313em;">&gt;</span><span class="mn" id="MathJax-Span-242" style="font-family: STIXGeneral; padding-left: 0.313em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.094em; border-left: 0px solid; width: 0px; height: 1.035em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>x</mi><mo>&gt;</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-25">x > 0</script> elementwise in <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-26-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-243" style="width: 6.541em; display: inline-block;"><span style="display: inline-block; position: relative; width: 5em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.579em, 1004.97em, 2.899em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-244"><span class="mi" id="MathJax-Span-245" style="font-family: STIXGeneral; font-style: italic; text-rendering: optimizelegibility;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span class="mo" id="MathJax-Span-246" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="msubsup" id="MathJax-Span-247" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 1.19em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.65em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-248" style="font-family: STIXGeneral; font-style: italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -4.353em; left: 0.667em;"><span class="mi" id="MathJax-Span-249" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mi" id="MathJax-Span-250" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-251" style="font-family: STIXGeneral; padding-left: 0.25em;">+</span><span class="mi" id="MathJax-Span-252" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.25em;">b</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.332em; border-left: 0px solid; width: 0px; height: 1.466em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo>=</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi></math></span></span><script type="math/tex" id="MathJax-Element-26">f = w^Tx + b</script>)), then the gradient on the weights <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-27-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-253" style="width: 0.916em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.673em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.963em, 1000.65em, 2.71em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-254"><span class="mi" id="MathJax-Span-255" style="font-family: STIXGeneral; font-style: italic;">w</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.086em; border-left: 0px solid; width: 0px; height: 0.722em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>w</mi></math></span></span><script type="math/tex" id="MathJax-Element-27">w</script> will during backpropagation become either all be positive, or all negative (depending on the gradient of the whole expression <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-28-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-256" style="width: 0.579em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.433em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.726em, 1000.43em, 2.899em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-257"><span class="mi" id="MathJax-Span-258" style="font-family: STIXGeneral; font-style: italic; text-rendering: optimizelegibility;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.332em; border-left: 0px solid; width: 0px; height: 1.276em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi></math></span></span><script type="math/tex" id="MathJax-Element-28">f</script>).
 This could introduce undesirable zig-zagging dynamics in the gradient 
updates for the weights. However, notice that once these gradients are 
added up across a batch of data the final update for the weights can 
have variable signs, somewhat mitigating this issue. Therefore, this is 
an inconvenience but it has less severe consequences compared to the 
saturated activation problem above.</li>
</ul>

<p><strong>Tanh.</strong> The tanh non-linearity is shown on the image 
above on the right. It squashes a real-valued number to the range [-1, 
1]. Like the sigmoid neuron, its activations saturate, but unlike the 
sigmoid neuron its output is zero-centered. Therefore, in practice the <em>tanh non-linearity is always preferred to the sigmoid nonlinearity.</em> Also note that the tanh neuron is simply a scaled sigmoid neuron, in particular the following holds: <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-29-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;tanh&lt;/mi&gt;&lt;mo&gt;&amp;#x2061;&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;&amp;#x03C3;&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-259" style="width: 10.82em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.317em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.721em, 1008.21em, 2.869em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-260"><span class="mi" id="MathJax-Span-261" style="font-family: STIXGeneral;">tanh</span><span class="mo" id="MathJax-Span-262"></span><span class="mo" id="MathJax-Span-263" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-264" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-265" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-266" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-267" style="font-family: STIXGeneral; padding-left: 0.313em;">2</span><span class="mi" id="MathJax-Span-268" style="font-family: STIXGeneral; font-style: italic;">σ<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.033em;"></span></span><span class="mo" id="MathJax-Span-269" style="font-family: STIXGeneral;">(</span><span class="mn" id="MathJax-Span-270" style="font-family: STIXGeneral;">2</span><span class="mi" id="MathJax-Span-271" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-272" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-273" style="font-family: STIXGeneral; padding-left: 0.25em;">−</span><span class="mn" id="MathJax-Span-274" style="font-family: STIXGeneral; padding-left: 0.25em;">1</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.293em; border-left: 0px solid; width: 0px; height: 1.243em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>tanh</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>2</mn><mi>σ</mi><mo stretchy="false">(</mo><mn>2</mn><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mn>1</mn></math></span></span><script type="math/tex" id="MathJax-Element-29"> \tanh(x) = 2 \sigma(2x) -1  </script>.</p>

<div class="fig figcenter fighighlight">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/relu.jpeg" width="40%">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/alexplot.jpeg" style="border-left: 1px solid black;" width="40%">
  <div class="figcaption"><b>Left:</b> Rectified Linear Unit (ReLU) 
activation function, which is zero when x &amp;lt 0 and then linear with
 slope 1 when x &amp;gt 0. <b>Right:</b> A plot from <a href="http://www.cs.toronto.edu/%7Efritz/absps/imagenet.pdf">Krizhevsky et al.</a> (pdf) paper indicating the 6x improvement in convergence with the ReLU unit compared to the tanh unit.</div>
</div>

<p><strong>ReLU.</strong> The Rectified Linear Unit has become very popular in the last few years. It computes the function <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-30-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-275" style="width: 8.512em; display: inline-block;"><span style="display: inline-block; position: relative; width: 6.538em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.726em, 1006.49em, 2.899em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-276"><span class="mi" id="MathJax-Span-277" style="font-family: STIXGeneral; font-style: italic; text-rendering: optimizelegibility;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span class="mo" id="MathJax-Span-278" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-279" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-280" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-281" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mo" id="MathJax-Span-282" style="font-family: STIXGeneral; padding-left: 0.313em;">max</span><span class="mo" id="MathJax-Span-283" style="font-family: STIXGeneral;">(</span><span class="mn" id="MathJax-Span-284" style="font-family: STIXGeneral;">0</span><span class="mo" id="MathJax-Span-285" style="font-family: STIXGeneral;">,</span><span class="mi" id="MathJax-Span-286" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.188em;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-287" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.332em; border-left: 0px solid; width: 0px; height: 1.276em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><mn>0</mn><mo>,</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-30">f(x) = \max(0, x)</script>.
 In other words, the activation is simply thresholded at zero (see image
 above on the left). There are several pros and cons to using the ReLUs:</p>

<ul>
  <li>(+) It was found to greatly accelerate (e.g. a factor of 6 in <a href="http://www.cs.toronto.edu/%7Efritz/absps/imagenet.pdf">Krizhevsky et al.</a>)
 the convergence of stochastic gradient descent compared to the 
sigmoid/tanh functions. It is argued that this is due to its linear, 
non-saturating form.</li>
  <li>(+) Compared to tanh/sigmoid neurons that involve expensive 
operations (exponentials, etc.), the ReLU can be implemented by simply 
thresholding a matrix of activations at zero.</li>
  <li>(-) Unfortunately, ReLU units can be fragile during training and 
can “die”. For example, a large gradient flowing through a ReLU neuron 
could cause the weights to update in such a way that the neuron will 
never activate on any datapoint again. If this happens, then the 
gradient flowing through the unit will forever be zero from that point 
on. That is, the ReLU units can irreversibly die during training since 
they can get knocked off the data manifold. For example, you may find 
that as much as 40% of your network can be “dead” (i.e. neurons that 
never activate across the entire training dataset) if the learning rate 
is set too high. With a proper setting of the learning rate this is less
 frequently an issue.</li>
</ul>

<p><strong>Leaky ReLU.</strong> Leaky ReLUs are one attempt to fix the 
“dying ReLU” problem. Instead of the function being zero when x &lt; 0, a
 leaky ReLU will instead have a small negative slope (of 0.01, or so). 
That is, the function computes <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-31-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn mathvariant=&quot;double-struck&quot;&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn mathvariant=&quot;double-struck&quot;&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-288" style="width: 18.56em; display: inline-block;"><span style="display: inline-block; position: relative; width: 14.279em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.711em, 1014.23em, 2.899em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-289"><span class="mi" id="MathJax-Span-290" style="font-family: STIXGeneral; font-style: italic; text-rendering: optimizelegibility;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span class="mo" id="MathJax-Span-291" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-292" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-293" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-294" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="texatom" id="MathJax-Span-295" style="padding-left: 0.313em;"><span class="mrow" id="MathJax-Span-296"><span class="mn" id="MathJax-Span-297" style="font-family: STIXGeneral;">𝟙</span></span></span><span class="mo" id="MathJax-Span-298" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-299" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-300" style="font-family: STIXGeneral; padding-left: 0.313em;">&lt;</span><span class="mn" id="MathJax-Span-301" style="font-family: STIXGeneral; padding-left: 0.313em;">0</span><span class="mo" id="MathJax-Span-302" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-303" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-304" style="font-family: STIXGeneral; font-style: italic;">α</span><span class="mi" id="MathJax-Span-305" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-306" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-307" style="font-family: STIXGeneral; padding-left: 0.25em;">+</span><span class="texatom" id="MathJax-Span-308" style="padding-left: 0.25em;"><span class="mrow" id="MathJax-Span-309"><span class="mn" id="MathJax-Span-310" style="font-family: STIXGeneral;">𝟙</span></span></span><span class="mo" id="MathJax-Span-311" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-312" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-313" style="font-family: STIXGeneral; padding-left: 0.313em;">&gt;<span style="font-family: STIXGeneral; font-style: normal; font-weight: normal;">=</span></span><span class="mn" id="MathJax-Span-314" style="font-family: STIXGeneral; padding-left: 0.313em;">0</span><span class="mo" id="MathJax-Span-315" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-316" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-317" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-318" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.332em; border-left: 0px solid; width: 0px; height: 1.295em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mrow class="MJX-TeXAtom-ORD"><mn mathvariant="double-struck">1</mn></mrow><mo stretchy="false">(</mo><mi>x</mi><mo>&lt;</mo><mn>0</mn><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>α</mi><mi>x</mi><mo stretchy="false">)</mo><mo>+</mo><mrow class="MJX-TeXAtom-ORD"><mn mathvariant="double-struck">1</mn></mrow><mo stretchy="false">(</mo><mi>x</mi><mo>&gt;=</mo><mn>0</mn><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-31">f(x) = \mathbb{1}(x < 0) (\alpha x) + \mathbb{1}(x>=0) (x) </script> where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-32-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03B1;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-319" style="width: 0.724em; display: inline-block;"><span style="display: inline-block; position: relative; width: 0.529em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.963em, 1000.53em, 2.703em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-320"><span class="mi" id="MathJax-Span-321" style="font-family: STIXGeneral; font-style: italic;">α</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.077em; border-left: 0px solid; width: 0px; height: 0.713em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>α</mi></math></span></span><script type="math/tex" id="MathJax-Element-32">\alpha</script>
 is a small constant. Some people report success with this form of 
activation function, but the results are not always consistent. The 
slope in the negative region can also be made into a parameter of each 
neuron, as seen in PReLU neurons, introduced in <a href="http://arxiv.org/abs/1502.01852">Delving Deep into Rectifiers</a>, by Kaiming He et al., 2015. However, the consistency of the benefit across tasks is presently unclear.</p>

<p><strong>Maxout</strong>. Other types of units have been proposed that do not have the functional form <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-33-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-322" style="width: 5.724em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.375em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.579em, 1004.33em, 2.899em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-323"><span class="mi" id="MathJax-Span-324" style="font-family: STIXGeneral; font-style: italic; text-rendering: optimizelegibility;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span class="mo" id="MathJax-Span-325" style="font-family: STIXGeneral;">(</span><span class="msubsup" id="MathJax-Span-326"><span style="display: inline-block; position: relative; width: 1.19em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.65em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-327" style="font-family: STIXGeneral; font-style: italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -4.353em; left: 0.667em;"><span class="mi" id="MathJax-Span-328" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mi" id="MathJax-Span-329" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-330" style="font-family: STIXGeneral; padding-left: 0.25em;">+</span><span class="mi" id="MathJax-Span-331" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.25em;">b</span><span class="mo" id="MathJax-Span-332" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.332em; border-left: 0px solid; width: 0px; height: 1.466em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><msup><mi>w</mi><mi>T</mi></msup><mi>x</mi><mo>+</mo><mi>b</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-33">f(w^Tx + b)</script>
 where a non-linearity is applied on the dot product between the weights
 and the data. One relatively popular choice is the Maxout neuron 
(introduced recently by <a href="http://www-etud.iro.umontreal.ca/%7Egoodfeli/maxout.html">Goodfellow et al.</a>) that generalizes the ReLU and its leaky version. The Maxout neuron computes the function <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-34-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mo movablelimits=&quot;true&quot; form=&quot;prefix&quot;&gt;max&lt;/mo&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msubsup&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/msubsup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-333" style="width: 13.32em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.24em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.598em, 1010.19em, 3.006em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-334"><span class="mo" id="MathJax-Span-335" style="font-family: STIXGeneral;">max</span><span class="mo" id="MathJax-Span-336" style="font-family: STIXGeneral;">(</span><span class="msubsup" id="MathJax-Span-337"><span style="display: inline-block; position: relative; width: 1.19em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.65em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-338" style="font-family: STIXGeneral; font-style: italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(3.384em, 1000.52em, 4.135em, -1000em); top: -4.335em; left: 0.667em;"><span class="mi" id="MathJax-Span-339" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(3.368em, 1000.43em, 4.135em, -1000em); top: -3.677em; left: 0.667em;"><span class="mn" id="MathJax-Span-340" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mi" id="MathJax-Span-341" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-342" style="font-family: STIXGeneral; padding-left: 0.25em;">+</span><span class="msubsup" id="MathJax-Span-343" style="padding-left: 0.25em;"><span style="display: inline-block; position: relative; width: 0.929em; height: 0px;"><span style="position: absolute; clip: rect(3.163em, 1000.47em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-344" style="font-family: STIXGeneral; font-style: italic;">b</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.5em;"><span class="mn" id="MathJax-Span-345" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-346" style="font-family: STIXGeneral;">,</span><span class="msubsup" id="MathJax-Span-347" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 1.19em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.65em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-348" style="font-family: STIXGeneral; font-style: italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(3.384em, 1000.52em, 4.135em, -1000em); top: -4.335em; left: 0.667em;"><span class="mi" id="MathJax-Span-349" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">T<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.054em;"></span></span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; clip: rect(3.368em, 1000.43em, 4.135em, -1000em); top: -3.677em; left: 0.667em;"><span class="mn" id="MathJax-Span-350" style="font-size: 70.7%; font-family: STIXGeneral;">2</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mi" id="MathJax-Span-351" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-352" style="font-family: STIXGeneral; padding-left: 0.25em;">+</span><span class="msubsup" id="MathJax-Span-353" style="padding-left: 0.25em;"><span style="display: inline-block; position: relative; width: 0.929em; height: 0px;"><span style="position: absolute; clip: rect(3.163em, 1000.47em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-354" style="font-family: STIXGeneral; font-style: italic;">b</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.5em;"><span class="mn" id="MathJax-Span-355" style="font-size: 70.7%; font-family: STIXGeneral;">2</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-356" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.47em; border-left: 0px solid; width: 0px; height: 1.581em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mo movablelimits="true" form="prefix">max</mo><mo stretchy="false">(</mo><msubsup><mi>w</mi><mn>1</mn><mi>T</mi></msubsup><mi>x</mi><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo>,</mo><msubsup><mi>w</mi><mn>2</mn><mi>T</mi></msubsup><mi>x</mi><mo>+</mo><msub><mi>b</mi><mn>2</mn></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-34">\max(w_1^Tx+b_1, w_2^Tx + b_2)</script>. Notice that both ReLU and Leaky ReLU are a special case of this form (for example, for ReLU we have <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-35-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-357" style="width: 5.531em; display: inline-block;"><span style="display: inline-block; position: relative; width: 4.231em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.721em, 1004.21em, 2.842em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-358"><span class="msubsup" id="MathJax-Span-359"><span style="display: inline-block; position: relative; width: 1.096em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.65em, 4.153em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-360" style="font-family: STIXGeneral; font-style: italic;">w</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.667em;"><span class="mn" id="MathJax-Span-361" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-362" style="font-family: STIXGeneral;">,</span><span class="msubsup" id="MathJax-Span-363" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 0.929em; height: 0px;"><span style="position: absolute; clip: rect(3.163em, 1000.47em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-364" style="font-family: STIXGeneral; font-style: italic;">b</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.5em;"><span class="mn" id="MathJax-Span-365" style="font-size: 70.7%; font-family: STIXGeneral;">1</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-366" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="mn" id="MathJax-Span-367" style="font-family: STIXGeneral; padding-left: 0.313em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.258em; border-left: 0px solid; width: 0px; height: 1.208em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><msub><mi>w</mi><mn>1</mn></msub><mo>,</mo><msub><mi>b</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-35">w_1, b_1 = 0</script>).
 The Maxout neuron therefore enjoys all the benefits of a ReLU unit 
(linear regime of operation, no saturation) and does not have its 
drawbacks (dying ReLU). However, unlike the ReLU neurons it doubles the 
number of parameters for every single neuron, leading to a high total 
number of parameters.</p>

<p>This concludes our discussion of the most common types of neurons and
 their activation functions. As a last comment, it is very rare to mix 
and match different types of neurons in the same network, even though 
there is no fundamental problem with doing so.</p>

<p><strong>TLDR</strong>: “<em>What neuron type should I use?</em>” Use 
the ReLU non-linearity, be careful with your learning rates and possibly
 monitor the fraction of “dead” units in a network. If this concerns 
you, give Leaky ReLU or Maxout a try. Never use sigmoid. Try tanh, but 
expect it to work worse than ReLU/Maxout.</p>

<p><a name="nn"></a></p>

<h2 id="neural-network-architectures">Neural Network architectures</h2>

<p><a name="layers"></a></p>

<h3 id="layer-wise-organization">Layer-wise organization</h3>

<p><strong>Neural Networks as neurons in graphs</strong>. Neural 
Networks are modeled as collections of neurons that are connected in an 
acyclic graph. In other words, the outputs of some neurons can become 
inputs to other neurons. Cycles are not allowed since that would imply 
an infinite loop in the forward pass of a network. Instead of an 
amorphous blobs of connected neurons, Neural Network models are often 
organized into distinct layers of neurons. For regular neural networks, 
the most common layer type is the <strong>fully-connected layer</strong>
 in which  neurons between two adjacent layers are fully pairwise 
connected, but neurons within a single layer share no connections. Below
 are two example Neural Network topologies that use a stack of 
fully-connected layers:</p>

<div class="fig figcenter fighighlight">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/neural_net.jpeg" width="40%">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/neural_net2.jpeg" style="border-left: 1px solid black;" width="55%">
  <div class="figcaption"><b>Left:</b> A 2-layer Neural Network (one hidden layer of 4 neurons (or units) and one output layer with 2 neurons), and three inputs. <b>Right:</b>
 A 3-layer neural network with three inputs, two hidden layers of 4 
neurons each and one output layer. Notice that in both cases there are 
connections (synapses) between neurons across layers, but not within a 
layer.</div>
</div>

<p><strong>Naming conventions.</strong> Notice that when we say N-layer 
neural network, we do not count the input layer. Therefore, a 
single-layer neural network describes a network with no hidden layers 
(input directly mapped to output). In that sense, you can sometimes hear
 people say that logistic regression or SVMs are simply a special case 
of single-layer Neural Networks. You may also hear these networks 
interchangeably referred to as <em>“Artificial Neural Networks”</em> (ANN) or <em>“Multi-Layer Perceptrons”</em> (MLP). Many people do not like the analogies between Neural Networks and real brains and prefer to refer to neurons as <em>units</em>.</p>

<p><strong>Output layer.</strong> Unlike all layers in a Neural Network,
 the output layer neurons most commonly do not have an activation 
function (or you can think of them as having a linear identity 
activation function). This is because the last output layer is usually 
taken to represent the class scores (e.g. in classification), which are 
arbitrary real-valued numbers, or some kind of real-valued target (e.g. 
in regression).</p>

<p><strong>Sizing neural networks</strong>. The two metrics that people 
commonly use to measure the size of neural networks are the number of 
neurons, or more commonly the number of parameters. Working with the two
 example networks in the above picture:</p>

<ul>
  <li>The first network (left) has 4 + 2 = 6 neurons (not counting the 
inputs), [3 x 4] + [4 x 2] = 20 weights and 4 + 2 = 6 biases, for a 
total of 26 learnable parameters.</li>
  <li>The second network (right) has 4 + 4 + 1 = 9 neurons, [3 x 4] + [4
 x 4] + [4 x 1] = 12 + 16 + 4 = 32 weights and 4 + 4 + 1 = 9 biases, for
 a total of 41 learnable parameters.</li>
</ul>

<p>To give you some context, modern Convolutional Networks contain on 
orders of 100 million parameters and are usually made up of 
approximately 10-20 layers (hence <em>deep learning</em>). However, as we will see the number of <em>effective</em> connections is significantly greater due to parameter sharing. More on this in the Convolutional Neural Networks module.</p>

<p><a name="feedforward"></a></p>

<h3 id="example-feed-forward-computation">Example feed-forward computation</h3>

<p><em>Repeated matrix multiplications interwoven with activation function</em>.
 One of the primary reasons that Neural Networks are organized into 
layers is that this structure makes it very simple and efficient to 
evaluate Neural Networks using matrix vector operations. Working with 
the example three-layer neural network in the diagram above, the input 
would be a [3x1] vector. All connection strengths for a layer can be 
stored in a single matrix. For example, the first hidden layer’s weights
 <code class="highlighter-rouge">W1</code> would be of size [4x3], and the biases for all units would be in the vector <code class="highlighter-rouge">b1</code>, of size [4x1]. Here, every single neuron has its weights in a row of <code class="highlighter-rouge">W1</code>, so the matrix vector multiplication <code class="highlighter-rouge">np.dot(W1,x)</code> evaluates the activations of all neurons in that layer. Similarly, <code class="highlighter-rouge">W2</code> would be a [4x4] matrix that stores the connections of the second hidden layer, and <code class="highlighter-rouge">W3</code>
 a [1x4] matrix for the last (output) layer. The full forward pass of 
this 3-layer neural network is then simply three matrix multiplications,
 interwoven with the application of the activation function:</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="c"># forward-pass of a 3-layer neural network:</span>
<span class="n">f</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="mf">1.0</span><span class="o">/</span><span class="p">(</span><span class="mf">1.0</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="c"># activation function (use sigmoid)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span> <span class="c"># random input vector of three numbers (3x1)</span>
<span class="n">h1</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span><span class="p">)</span> <span class="c"># calculate first hidden layer activations (4x1)</span>
<span class="n">h2</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">,</span> <span class="n">h1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span> <span class="c"># calculate second hidden layer activations (4x1)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W3</span><span class="p">,</span> <span class="n">h2</span><span class="p">)</span> <span class="o">+</span> <span class="n">b3</span> <span class="c"># output neuron (1x1)</span>
</code></pre>
</div>

<p>In the above code, <code class="highlighter-rouge">W1,W2,W3,b1,b2,b3</code> are the learnable parameters of the network. Notice also that instead of having a single input column vector, the variable <code class="highlighter-rouge">x</code> could hold an entire batch of training data (where each input example would be a column of <code class="highlighter-rouge">x</code>)
 and then all examples would be efficiently evaluated in parallel. 
Notice that the final Neural Network layer usually doesn’t have an 
activation function (e.g. it represents a (real-valued) class score in a
 classification setting).</p>

<blockquote>
  <p>The forward pass of a fully-connected layer corresponds to one 
matrix multiplication followed by a bias offset and an activation 
function.</p>
</blockquote>

<p><a name="power"></a></p>

<h3 id="representational-power">Representational power</h3>

<p>One way to look at Neural Networks with fully-connected layers is 
that they define a family of functions that are parameterized by the 
weights of the network. A natural question that arises is: What is the 
representational power of this family of functions? In particular, are 
there functions that cannot be modeled with a Neural Network?</p>

<p>It turns out that Neural Networks with at least one hidden layer are <em>universal approximators</em>. That is, it can be shown (e.g. see <a href="http://www.dartmouth.edu/%7Egvc/Cybenko_MCSS.pdf"><em>Approximation by Superpositions of Sigmoidal Function</em></a> from 1989 (pdf), or this <a href="http://neuralnetworksanddeeplearning.com/chap4.html">intuitive explanation</a> from Michael Nielsen) that given any continuous function <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-36-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-368" style="width: 2.022em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.538em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.726em, 1001.49em, 2.899em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-369"><span class="mi" id="MathJax-Span-370" style="font-family: STIXGeneral; font-style: italic; text-rendering: optimizelegibility;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span class="mo" id="MathJax-Span-371" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-372" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-373" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.332em; border-left: 0px solid; width: 0px; height: 1.276em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-36">f(x)</script> and some <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-37-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;mo&gt;&amp;gt;&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-374" style="width: 2.887em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.212em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.728em, 1002.19em, 2.716em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-375"><span class="mi" id="MathJax-Span-376" style="font-family: STIXGeneral; font-style: italic;">ϵ</span><span class="mo" id="MathJax-Span-377" style="font-family: STIXGeneral; padding-left: 0.313em;">&gt;</span><span class="mn" id="MathJax-Span-378" style="font-family: STIXGeneral; padding-left: 0.313em;">0</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.094em; border-left: 0px solid; width: 0px; height: 1.035em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>ϵ</mi><mo>&gt;</mo><mn>0</mn></math></span></span><script type="math/tex" id="MathJax-Element-37">\epsilon > 0</script>, there exists a Neural Network <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-38-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-379" style="width: 2.07em; display: inline-block;"><span style="display: inline-block; position: relative; width: 1.587em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.728em, 1001.54em, 2.898em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-380"><span class="mi" id="MathJax-Span-381" style="font-family: STIXGeneral; font-style: italic;">g</span><span class="mo" id="MathJax-Span-382" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-383" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-384" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.33em; border-left: 0px solid; width: 0px; height: 1.272em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-38">g(x)</script> with one hidden layer (with a reasonable choice of non-linearity, e.g. sigmoid) such that <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-39-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;&amp;#x2200;&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;&amp;#x2223;&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2212;&lt;/mo&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;&amp;#x2223;&amp;lt;&lt;/mo&gt;&lt;mi&gt;&amp;#x03F5;&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-385" style="width: 10.916em; display: inline-block;"><span style="display: inline-block; position: relative; width: 8.365em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.714em, 1008.34em, 2.899em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-386"><span class="mi" id="MathJax-Span-387" style="font-family: STIXGeneral;">∀</span><span class="mi" id="MathJax-Span-388" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-389" style="font-family: STIXGeneral;">,</span><span class="mo" id="MathJax-Span-390" style="font-family: STIXGeneral; padding-left: 0.188em;">∣</span><span class="mi" id="MathJax-Span-391" style="font-family: STIXGeneral; font-style: italic; text-rendering: optimizelegibility; padding-left: 0.313em;">f<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.146em;"></span></span><span class="mo" id="MathJax-Span-392" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-393" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-394" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-395" style="font-family: STIXGeneral; padding-left: 0.25em;">−</span><span class="mi" id="MathJax-Span-396" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.25em;">g</span><span class="mo" id="MathJax-Span-397" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-398" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-399" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-400" style="font-family: STIXGeneral; padding-left: 0.313em;">∣<span style="font-family: STIXGeneral; font-style: normal; font-weight: normal;">&lt;</span></span><span class="mi" id="MathJax-Span-401" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.313em;">ϵ</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.332em; border-left: 0px solid; width: 0px; height: 1.291em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi mathvariant="normal">∀</mi><mi>x</mi><mo>,</mo><mo>∣</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>−</mo><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∣&lt;</mo><mi>ϵ</mi></math></span></span><script type="math/tex" id="MathJax-Element-39"> \forall x, \mid f(x) - g(x) \mid < \epsilon </script>. In other words, the neural network can approximate any continuous function.</p>

<p>If one hidden layer suffices to approximate any function, why use 
more layers and go deeper? The answer is that the fact that a two-layer 
Neural Network is a universal approximator is, while mathematically 
cute, a relatively weak and useless statement in practice. In one 
dimension, the “sum of indicator bumps” function <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-40-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;munder&gt;&lt;mo&gt;&amp;#x2211;&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/munder&gt;&lt;msub&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mrow class=&quot;MJX-TeXAtom-ORD&quot;&gt;&lt;mn mathvariant=&quot;double-struck&quot;&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mo stretchy=&quot;false&quot;&gt;(&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;&amp;lt;&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo stretchy=&quot;false&quot;&gt;)&lt;/mo&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-402" style="width: 14.041em; display: inline-block;"><span style="display: inline-block; position: relative; width: 10.769em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.643em, 1010.72em, 2.996em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-403"><span class="mi" id="MathJax-Span-404" style="font-family: STIXGeneral; font-style: italic;">g</span><span class="mo" id="MathJax-Span-405" style="font-family: STIXGeneral;">(</span><span class="mi" id="MathJax-Span-406" style="font-family: STIXGeneral; font-style: italic;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-407" style="font-family: STIXGeneral;">)</span><span class="mo" id="MathJax-Span-408" style="font-family: STIXGeneral; padding-left: 0.313em;">=</span><span class="munderover" id="MathJax-Span-409" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 1.186em; height: 0px;"><span style="position: absolute; clip: rect(3.085em, 1000.86em, 4.396em, -1000em); top: -3.99em; left: 0em;"><span class="mo" id="MathJax-Span-410" style="font-family: STIXGeneral; vertical-align: -0.002em;">∑</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.694em; left: 0.914em;"><span class="mi" id="MathJax-Span-411" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="msubsup" id="MathJax-Span-412" style="padding-left: 0.188em;"><span style="display: inline-block; position: relative; width: 0.716em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.42em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-413" style="font-family: STIXGeneral; font-style: italic;">c</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.444em;"><span class="mi" id="MathJax-Span-414" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="texatom" id="MathJax-Span-415"><span class="mrow" id="MathJax-Span-416"><span class="mn" id="MathJax-Span-417" style="font-family: STIXGeneral;">𝟙</span></span></span><span class="mo" id="MathJax-Span-418" style="font-family: STIXGeneral;">(</span><span class="msubsup" id="MathJax-Span-419"><span style="display: inline-block; position: relative; width: 0.773em; height: 0px;"><span style="position: absolute; clip: rect(3.405em, 1000.48em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-420" style="font-family: STIXGeneral; font-style: italic;">a</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.501em;"><span class="mi" id="MathJax-Span-421" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-422" style="font-family: STIXGeneral; padding-left: 0.313em;">&lt;</span><span class="mi" id="MathJax-Span-423" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.313em;">x<span style="display: inline-block; overflow: hidden; height: 1px; width: 0.003em;"></span></span><span class="mo" id="MathJax-Span-424" style="font-family: STIXGeneral; padding-left: 0.313em;">&lt;</span><span class="msubsup" id="MathJax-Span-425" style="padding-left: 0.313em;"><span style="display: inline-block; position: relative; width: 0.772em; height: 0px;"><span style="position: absolute; clip: rect(3.163em, 1000.47em, 4.146em, -1000em); top: -3.99em; left: 0em;"><span class="mi" id="MathJax-Span-426" style="font-family: STIXGeneral; font-style: italic;">b</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span><span style="position: absolute; top: -3.84em; left: 0.5em;"><span class="mi" id="MathJax-Span-427" style="font-size: 70.7%; font-family: STIXGeneral; font-style: italic;">i</span><span style="display: inline-block; width: 0px; height: 3.99em;"></span></span></span></span><span class="mo" id="MathJax-Span-428" style="font-family: STIXGeneral;">)</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.458em; border-left: 0px solid; width: 0px; height: 1.51em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>g</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><munder><mo>∑</mo><mi>i</mi></munder><msub><mi>c</mi><mi>i</mi></msub><mrow class="MJX-TeXAtom-ORD"><mn mathvariant="double-struck">1</mn></mrow><mo stretchy="false">(</mo><msub><mi>a</mi><mi>i</mi></msub><mo>&lt;</mo><mi>x</mi><mo>&lt;</mo><msub><mi>b</mi><mi>i</mi></msub><mo stretchy="false">)</mo></math></span></span><script type="math/tex" id="MathJax-Element-40">g(x) = \sum_i c_i \mathbb{1}(a_i < x < b_i)</script> where <span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="MathJax" id="MathJax-Element-41-Frame" tabindex="0" style="position: relative;" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;/math&gt;" role="presentation"><nobr aria-hidden="true"><span class="math" id="MathJax-Span-429" style="width: 2.935em; display: inline-block;"><span style="display: inline-block; position: relative; width: 2.26em; height: 0px; font-size: 130%;"><span style="position: absolute; clip: rect(1.721em, 1002.24em, 2.833em, -1000em); top: -2.548em; left: 0em;"><span class="mrow" id="MathJax-Span-430"><span class="mi" id="MathJax-Span-431" style="font-family: STIXGeneral; font-style: italic;">a</span><span class="mo" id="MathJax-Span-432" style="font-family: STIXGeneral;">,</span><span class="mi" id="MathJax-Span-433" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.188em;">b</span><span class="mo" id="MathJax-Span-434" style="font-family: STIXGeneral;">,</span><span class="mi" id="MathJax-Span-435" style="font-family: STIXGeneral; font-style: italic; padding-left: 0.188em;">c</span></span><span style="display: inline-block; width: 0px; height: 2.548em;"></span></span></span><span style="display: inline-block; overflow: hidden; vertical-align: -0.246em; border-left: 0px solid; width: 0px; height: 1.196em;"></span></span></nobr><span class="MJX_Assistive_MathML" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML"><mi>a</mi><mo>,</mo><mi>b</mi><mo>,</mo><mi>c</mi></math></span></span><script type="math/tex" id="MathJax-Element-41">a,b,c</script>
 are parameter vectors is also a universal approximator, but noone would
 suggest that we use this functional form in Machine Learning. Neural 
Networks work well in practice because they compactly express nice, 
smooth functions that fit well with the statistical properties of data 
we encounter in practice, and are also easy to learn using our 
optimization algorithms (e.g. gradient descent). Similarly, the fact 
that deeper networks (with multiple hidden layers) can work better than a
 single-hidden-layer networks is an empirical observation, despite the 
fact that their representational power is equal.</p>

<p>As an aside, in practice it is often the case that 3-layer neural 
networks will outperform 2-layer nets, but going even deeper 
(4,5,6-layer) rarely helps much more. This is in stark contrast to 
Convolutional Networks, where depth has been found to be an extremely 
important component for a good recognition system (e.g. on order of 10 
learnable layers). One argument for this observation is that images 
contain hierarchical structure (e.g. faces are made up of eyes, which 
are made up of edges, etc.), so several layers of processing make 
intuitive sense for this data domain.</p>

<p>The full story is, of course, much more involved and a topic of much 
recent research. If you are interested in these topics we recommend for 
further reading:</p>

<ul>
  <li><a href="http://www.deeplearningbook.org/">Deep Learning</a> book in press by Bengio, Goodfellow, Courville, in practicular <a href="http://www.deeplearningbook.org/contents/mlp.html">Chapter 6.4</a>.</li>
  <li><a href="http://arxiv.org/abs/1312.6184">Do Deep Nets Really Need to be Deep?</a></li>
  <li><a href="http://arxiv.org/abs/1412.6550">FitNets: Hints for Thin Deep Nets</a></li>
</ul>

<p><a name="arch"></a></p>

<h3 id="setting-number-of-layers-and-their-sizes">Setting number of layers and their sizes</h3>

<p>How do we decide on what architecture to use when faced with a 
practical problem? Should we use no hidden layers? One hidden layer? Two
 hidden layers? How large should each layer be? First, note that as we 
increase the size and number of layers in a Neural Network, the <strong>capacity</strong>
 of the network increases. That is, the space of representable functions
 grows since the neurons can collaborate to express many different 
functions. For example, suppose we had a binary classification problem 
in two dimensions. We could train three separate neural networks, each 
with one hidden layer of some size and obtain the following classifiers:</p>

<div class="fig figcenter fighighlight">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/layer_sizes.jpeg">
  <div class="figcaption">Larger Neural Networks can represent more 
complicated functions. The data are shown as circles colored by their 
class, and the decision regions by a trained neural network are shown 
underneath. You can play with these examples in this <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetsJS demo</a>.</div>
</div>

<p>In the diagram above, we can see that Neural Networks with more 
neurons can express more complicated functions. However, this is both a 
blessing (since we can learn to classify more complicated data) and a 
curse (since it is easier to overfit the training data). <strong>Overfitting</strong>
 occurs when a model with high capacity fits the noise in the data 
instead of the (assumed) underlying relationship. For example, the model
 with 20 hidden neurons fits all the training data but at the cost of 
segmenting the space into many disjoint red and green decision regions. 
The model with 3 hidden neurons only has the representational power to 
classify the data in broad strokes. It models the data as two blobs and 
interprets the few red points inside the green cluster as <strong>outliers</strong> (noise). In practice, this could lead to better <strong>generalization</strong> on the test set.</p>

<p>Based on our discussion above, it seems that smaller neural networks 
can be preferred if the data is not complex enough to prevent 
overfitting. However, this is incorrect - there are many other preferred
 ways to prevent overfitting in Neural Networks that we will discuss 
later (such as L2 regularization, dropout, input noise). In practice, it
 is always better to use these methods to control overfitting instead of
 the number of neurons.</p>

<p>The subtle reason behind this is that smaller networks are harder to 
train with local methods such as Gradient Descent: It’s clear that their
 loss functions have relatively few local minima, but it turns out that 
many of these minima are easier to converge to, and that they are bad 
(i.e. with high loss). Conversely, bigger neural networks contain 
significantly more local minima, but these minima turn out to be much 
better in terms of their actual loss. Since Neural Networks are 
non-convex, it is hard to study these properties mathematically, but 
some attempts to understand these objective functions have been made, 
e.g. in a recent paper <a href="http://arxiv.org/abs/1412.0233">The Loss Surfaces of Multilayer Networks</a>.
 In practice, what you find is that if you train a small network the 
final loss can display a good amount of variance - in some cases you get
 lucky and converge to a good place but in some cases you get trapped in
 one of the bad minima. On the other hand, if you train a large network 
you’ll start to find many different solutions, but the variance in the 
final achieved loss will be much smaller. In other words, all solutions 
are about equally as good, and rely less on the luck of random 
initialization.</p>

<p>To reiterate, the regularization strength is the preferred way to 
control the overfitting of a neural network. We can look at the results 
achieved by three different settings:</p>

<div class="fig figcenter fighighlight">
  <img src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/reg_strengths.jpeg">
  <div class="figcaption">
    The effects of regularization strength: Each neural network above 
has 20 hidden neurons, but changing the regularization strength makes 
its final decision regions smoother with a higher regularization. You 
can play with these examples in this <a href="http://cs.stanford.edu/people/karpathy/convnetjs/demo/classify2d.html">ConvNetsJS demo</a>.
  </div>
</div>

<p>The takeaway is that you should not be using smaller networks because
 you are afraid of overfitting. Instead, you should use as big of a 
neural network as your computational budget allows, and use other 
regularization techniques to control overfitting.</p>

<p><a name="summary"></a></p>

<h2 id="summary">Summary</h2>

<p>In summary,</p>

<ul>
  <li>We introduced a very coarse model of a biological <strong>neuron</strong></li>
  <li>We discussed several types of <strong>activation functions</strong> that are used in practice, with ReLU being the most common choice</li>
  <li>We introduced <strong>Neural Networks</strong> where neurons are connected with <strong>Fully-Connected layers</strong> where neurons in adjacent layers have full pair-wise connections, but neurons within a layer are not connected.</li>
  <li>We saw that this layered architecture enables very efficient 
evaluation of Neural Networks based on matrix multiplications interwoven
 with the application of the activation function.</li>
  <li>We saw that that Neural Networks are <strong>universal function approximators</strong>,
 but we also discussed the fact that this property has little to do with
 their ubiquitous use. They are used because they make certain “right” 
assumptions about the functional forms of functions that come up in 
practice.</li>
  <li>We discussed the fact that larger networks will always work better
 than smaller networks, but their higher model capacity must be 
appropriately addressed with stronger regularization (such as higher 
weight decay), or they might overfit. We will see more forms of 
regularization (especially dropout) in later sections.</li>
</ul>

<p><a name="add"></a></p>

<h2 id="additional-references">Additional References</h2>

<ul>
  <li><a href="http://www.deeplearning.net/tutorial/mlp.html">deeplearning.net tutorial</a> with Theano</li>
  <li><a href="http://cs.stanford.edu/people/karpathy/convnetjs/">ConvNetJS</a> demos for intuitions</li>
  <li><a href="http://neuralnetworksanddeeplearning.com/chap1.html">Michael Nielsen’s</a> tutorials</li>
</ul>

  </article>

</div>
      </div>
    </div>

    <footer class="site-footer">

  <div class="wrap">

    <div class="footer-col-1 column">
      <ul>
        
        <li>
          <a href="https://github.com/cs231n">
            <span class="icon github">
              <svg version="1.1" class="github-icon-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill-rule="evenodd" clip-rule="evenodd" fill="#C2C2C2" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761
                c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32
                c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472
                c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037
                C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65
                c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261
                c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082
                c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129
                c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"></path>
              </svg>
            </span>
            <span class="username">cs231n</span>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/cs231n">
            <span class="icon twitter">
              <svg version="1.1" class="twitter-icon-svg" xmlns="http://www.w3.org/2000/svg" xlink="http://www.w3.org/1999/xlink" x="0px" y="0px" viewBox="0 0 16 16" enable-background="new 0 0 16 16" xml:space="preserve">
                <path fill="#C2C2C2" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809
                c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27
                c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767
                c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206
                C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271
                c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469
                c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"></path>
              </svg>
            </span>
            <span class="username">cs231n</span>
          </a>
        </li>
        <li>
          <a href="mailto:karpathy@cs.stanford.edu">karpathy@cs.stanford.edu</a>
        </li>
      </ul>
    </div>

    <div class="footer-col-2 column">
        
    </div>

    <div class="footer-col-3 column">
      
    </div>

  </div>

</footer>


    <!-- mathjax -->
    <script type="text/javascript" src="2%20CS231n%20Convolutional%20Neural%20Networks%20for%20Visual%20Recognition_files/MathJax.js"></script>
    
<div style="position: absolute; width: 0px; height: 0px; overflow: hidden; padding: 0px; border: 0px none; margin: 0px;"><div id="MathJax_Font_Test" style="position: absolute; visibility: hidden; top: 0px; left: 0px; width: auto; padding: 0px; border: 0px none; margin: 0px; white-space: nowrap; text-align: left; text-indent: 0px; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; font-size: 40px; font-weight: normal; font-style: normal; font-size-adjust: none; font-family: STIXSizeOneSym,sans-serif;"></div></div></body></html>